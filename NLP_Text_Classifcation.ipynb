{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Text Classifcation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUhOn5Zio2YN"
      },
      "source": [
        "## NLP for Text Classification \r\n",
        "\r\n",
        "## **`Spam-Ham Classifier`**\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70QdZGrNpA5V",
        "outputId": "aadcd4bf-7d95-421e-8bbf-0f9ce6347c58"
      },
      "source": [
        "## importing necessary libraries\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "## Data Preprocessing libraries\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords ## for removal of words that are of no use in classification\r\n",
        "from nltk.stem.porter import PorterStemmer ## Stem to base words\r\n",
        "\r\n",
        "\r\n",
        "# XGBoost\r\n",
        "import xgboost as xgb\r\n",
        "from xgboost import XGBClassifier\r\n",
        "\r\n",
        "# sklearn \r\n",
        "from sklearn import model_selection\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\r\n",
        "from sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JHHIZaNpzje"
      },
      "source": [
        "Data provided has 2 columnns separated by tab. \r\n",
        "The first gives the information about the target label whether the message is Ham or Spam while the 2nd column contains the original message. Let's start by loading hte data and have a look at some messages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gWJkmj3ptI8"
      },
      "source": [
        "data = pd.read_csv('/content/SMSSpamCollection',sep='\\t',names = ['label','msg'])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "R0yvhlfvqbpI",
        "outputId": "69290421-8b9e-4221-b78c-353184cec3b3"
      },
      "source": [
        "data.sample(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1126</th>\n",
              "      <td>spam</td>\n",
              "      <td>For taking part in our mobile survey yesterday...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3743</th>\n",
              "      <td>ham</td>\n",
              "      <td>Hey i'm bored... So i'm thinking of u... So wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1905</th>\n",
              "      <td>ham</td>\n",
              "      <td>Wah... Okie okie... Muz make use of e unlimite...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>ham</td>\n",
              "      <td>Well, I have to leave for my class babe ... Yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5040</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pls clarify back if an open return ticket that...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     label                                                msg\n",
              "1126  spam  For taking part in our mobile survey yesterday...\n",
              "3743   ham  Hey i'm bored... So i'm thinking of u... So wa...\n",
              "1905   ham  Wah... Okie okie... Muz make use of e unlimite...\n",
              "1999   ham  Well, I have to leave for my class babe ... Yo...\n",
              "5040   ham  Pls clarify back if an open return ticket that..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB6tBi10rEex",
        "outputId": "a45781de-59fd-48ef-b294-ad3da8fd1b87"
      },
      "source": [
        "pd.set_option(\"display.max_colwidth\",-1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gpTcPDO6qdYf",
        "outputId": "593c9dd0-1a50-4b43-c9be-566bf8838d3b"
      },
      "source": [
        "## check how the messages look for both the labels\r\n",
        "data[data['label']=='ham']['msg'].values[7]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hGsan1Ecqy3Q",
        "outputId": "fe1d2cde-ead0-4e7b-8c50-84b40cb466cb"
      },
      "source": [
        "data[data['label']=='spam']['msg'].values[7]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/Ãº1.20 POBOXox36504W45WQ 16+'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GKCdl-vsV4l"
      },
      "source": [
        "From above messages we can clearly see how our data look like and more specifically what text does the spam and ham messages body contains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvbRL6xUtrBj"
      },
      "source": [
        "**Text Preprocessing**\r\n",
        "\r\n",
        "Now we are going to transform our data to the format required by our model. We will be performing number of steps Data cleaning, tokenizing , stopwords removal, converting to lower case, converting words to base form by stemming and at last converting to vectors using the below helper function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHDJDRT9tQac"
      },
      "source": [
        "ps = PorterStemmer()\r\n",
        "corpus = []\r\n",
        "for i in range(0, len(data)):\r\n",
        "    review = re.sub('[^a-zA-Z]', ' ', data['msg'][i])   ## selecting only the words\r\n",
        "    review = review.lower() ## converting all words to lowercase to eliminate any duplicate words\r\n",
        "    review = review.split() ## tokenizing\r\n",
        "    \r\n",
        "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')] ## converting words to their base form\r\n",
        "    review = ' '.join(review)\r\n",
        "    corpus.append(review)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzfiKXjavOGp",
        "outputId": "82402a92-8670-4246-f945-a863ff32ce40"
      },
      "source": [
        "## how does our corpus look after the text preprocessing\r\n",
        "corpus[:5]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['go jurong point crazi avail bugi n great world la e buffet cine got amor wat',\n",
              " 'ok lar joke wif u oni',\n",
              " 'free entri wkli comp win fa cup final tkt st may text fa receiv entri question std txt rate c appli',\n",
              " 'u dun say earli hor u c alreadi say',\n",
              " 'nah think goe usf live around though']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8WuyScDvl2P"
      },
      "source": [
        "**Time to convert to vectors using Bag of Words Model**\r\n",
        "\r\n",
        "We will using Count Vectorizer to implement BOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuVYNaqsvTNP"
      },
      "source": [
        "# Creating the Bag of Words model\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "cv = CountVectorizer(max_features=2500)\r\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km0cTwTzwAXg"
      },
      "source": [
        "X = cv.fit_transform(corpus).toarray()    "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqax4avwwF4I",
        "outputId": "f7408294-99b2-4d2c-f782-9cd9b7b0c8ba"
      },
      "source": [
        "## dimension of our independent data\r\n",
        "\r\n",
        "X.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5572, 2500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCGVuQcdwKgg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tggF3wVJwO3g"
      },
      "source": [
        "## words present in our vocab\r\n",
        "cv.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IY4OaskwSgn"
      },
      "source": [
        "## dependent data\r\n",
        "y = pd.get_dummies(data['label'])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "fM6tL9vHwy1R",
        "outputId": "ec61b55a-ff9b-48ea-d13a-2543b6330c54"
      },
      "source": [
        "y[:5]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ham</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ham  spam\n",
              "0  1    0   \n",
              "1  1    0   \n",
              "2  0    1   \n",
              "3  1    0   \n",
              "4  1    0   "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEF0_dOzwzZY"
      },
      "source": [
        "y = y.iloc[:,1].values"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA6XaHwdxD5F"
      },
      "source": [
        "### Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk5SUInzxAIA"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVgB4_YVxqbc"
      },
      "source": [
        "## Building a Text Classification Model\r\n",
        "\r\n",
        "Now the data is ready to be fed into a classification model. Let's create a basic claasification model using commonly used classification algorithms and see how our model performs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ0Wu7IZxLVZ",
        "outputId": "6a4159f1-9103-4346-d2e0-64274468d445"
      },
      "source": [
        "# Fitting a simple Logistic Regression on Counts\r\n",
        "clf = LogisticRegression(C=1.0)\r\n",
        "scores = model_selection.cross_val_score(clf, X_train, y_train, cv=5, scoring=\"f1\")\r\n",
        "scores"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.92035398, 0.90232558, 0.93273543, 0.90232558, 0.9321267 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Ka3Pyj0P1T"
      },
      "source": [
        "clf.fit(X_train, y_train)\r\n",
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThUtgApu0n7y",
        "outputId": "031c669c-f026-471d-f445-e56b3ec812ce"
      },
      "source": [
        "print(metrics.accuracy_score(y_pred,y_test))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9847533632286996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFml_uV3yRUv",
        "outputId": "a4c44a2f-f8e7-4924-e079-0ccfd2b6a484"
      },
      "source": [
        "# Fitting a simple Naive Bayes on Counts\r\n",
        "clf_NB = MultinomialNB()\r\n",
        "scores = model_selection.cross_val_score(clf_NB, X_train, y_train, cv=5, scoring=\"f1\")\r\n",
        "scores"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.95798319, 0.90295359, 0.93670886, 0.925     , 0.93220339])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeF9e7HL1Baw",
        "outputId": "b0a70891-bf99-4ede-9964-fdfbf9c6ae9d"
      },
      "source": [
        "clf_NB.fit(X_train, y_train)\r\n",
        "y_pred_NB = clf_NB.predict(X_test)\r\n",
        "print(metrics.accuracy_score(y_pred_NB,y_test))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9856502242152466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycw2iLflyYLQ",
        "outputId": "0e73fbb9-198f-46c0-8ac7-f642f363dacd"
      },
      "source": [
        "# Fitting a XGBoost on Counts\r\n",
        "import xgboost as xgb\r\n",
        "clf_xgb = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \r\n",
        "                        subsample=0.8, nthread=10, learning_rate=0.1)\r\n",
        "scores = model_selection.cross_val_score(clf_xgb, X_train, y_train, cv=5, scoring=\"f1\")\r\n",
        "scores"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.92982456, 0.89719626, 0.91818182, 0.91402715, 0.92857143])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeQA_4Lxymgo",
        "outputId": "13e2423d-7d2c-4bca-ad6d-4408da8cfb87"
      },
      "source": [
        "clf_xgb.fit(X_train, y_train)\r\n",
        "y_pred_xgb = clf_xgb.predict(X_test)\r\n",
        "print(metrics.accuracy_score(y_pred_xgb,y_test))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9856502242152466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YCk-87l10CI"
      },
      "source": [
        "So with basic models we are able to achieve accuracy of more than 98%. In the next part of notebook we will be extending the text classification part by applying other Bag of words approaches  -TfidfVectorizer and Hashing Vectorizer to increase our accuracy further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO3I6E9z2ixk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
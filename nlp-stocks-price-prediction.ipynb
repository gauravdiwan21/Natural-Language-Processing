{"cells":[{"metadata":{"id":"-c_Wt7sLx3sj"},"cell_type":"markdown","source":"# <font color='green'>Predicting Stocks prices using News Headlines</font>"},{"metadata":{},"cell_type":"markdown","source":"### * The kernel is all about creating a model to predict the stocks whether they go up or down based on the top 25 headlines \n \n### * The first column is \"Date\", the second is \"Label\", and the following ones are news headlines ranging from \"Top1\" to \"Top25\".\n \n### * In Label column the value is \"1\" when DJIA Adj Close value rose or stayed as the same\n \n### * In Label column the value is \"0\" when DJIA Adj Close value decreased."},{"metadata":{"id":"wNqgNvFvx3sl"},"cell_type":"markdown","source":"## <font color='darkred'>Objective :</font>\n### The goal is to create a machine learning model that predicts whether the stock goes up or down based on top 25 headlines "},{"metadata":{"id":"Lq19VS7yx3sm"},"cell_type":"markdown","source":"## <font color='darkred'>Whole process in detail :</font>\n### 1)  Filling null values in the dataset with median\n\n### 2)  Combining all the headlines into one news \n\n### 3)  Cleaning the text by removing punctuations and changing all the letters to lowercase\n\n### 4)  Applying countvectorizer to all the headlines\n\n### 5)  Visualizing the results and choosing the best algorithm based on requirements"},{"metadata":{"id":"MPcd6pZ1x3sn","outputId":"e68e24e5-70c3-440e-c274-3297fbfba296","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n\nwarnings.filterwarnings('ignore')\n\n# imported the file which contains top 25 headlines, stock went up or down(label) and date\ndata1 = pd.read_csv('../input/stocknews/Combined_News_DJIA.csv')\ndata1.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"gLg-cV1bx3su","outputId":"2d4e746f-1cb0-4f75-dfd7-076525d0a5ef","trusted":true},"cell_type":"code","source":"data1.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color='darkred'>Data Cleaning</font>"},{"metadata":{"id":"vcCMCU-0x3sy","trusted":true},"cell_type":"code","source":"# filling the null values with median \n\ndata1['Top23'].fillna(data1['Top23'].median,inplace=True)\ndata1['Top24'].fillna(data1['Top24'].median,inplace=True)\ndata1['Top25'].fillna(data1['Top25'].median,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train-Test Split "},{"metadata":{"id":"timWRR1Rx3s2","trusted":true},"cell_type":"code","source":"# seperating the data into train and test on date\n\ntrain = data1[data1['Date'] < '20150101']\ntest = data1[data1['Date'] > '20141231']","execution_count":null,"outputs":[]},{"metadata":{"id":"G8whKl3tx3s5","outputId":"f49253a3-8960-42d7-ca37-73b8f494e587","trusted":true},"cell_type":"code","source":"# removing punctuations and changing all the letters to lowercase for both train and test\n\nall_data = [train,test]\n\nfor df in all_data:\n    df.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)\n    for i in df.columns:\n        if i=='Date':\n            continue\n        if i=='Label':\n            continue\n        df[i] = df[i].str.lower()\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"OzpyMEfMx3s9","outputId":"ba650cf5-f648-4c61-b841-6fd998cab42c","trusted":true},"cell_type":"code","source":"# combining all the headlines in train data into one and appending them into a list \n\nheadlines = []\nfor row in range(0,len(train.index)):\n    headlines.append(' '.join(str(x) for x in train.iloc[row,2:]))\nheadlines[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"rawwemdUx3tQ","trusted":true},"cell_type":"code","source":"# combining all the headlines in test data into one and appending them into a list \n\ntest_transform= []\nfor row in range(0,len(test.index)):\n    test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color='darkred'>Applying Machine Learning Algorithms (Random forest , XGBOOST and CATBoost)</font>"},{"metadata":{"id":"QHPZUDSpx3tr","outputId":"5e3e9d5b-b39c-43e1-b880-c4b719caa486","trusted":true},"cell_type":"code","source":"# Applying countvectorizer on headlines list that we created before and max features is set to 100009\n\ncountvector=CountVectorizer(ngram_range=(2,2),max_features=100009)\ntraindataset=countvector.fit_transform(headlines)\n\nrandomclassifier=RandomForestClassifier(n_estimators=200,criterion='entropy')\nrandomclassifier.fit(traindataset,train['Label'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n<font color='darkblue'>The maximum features for countvectorizer is set to 100009 because, i tried many other numbers for maximum features and for 100009 i got the best accuracy, with lowest False positive values ( you can see below in the confusion matrix you can try other values and check it yourself, if you find the best accuracy with other maximum features then comment below</font>"},{"metadata":{},"cell_type":"markdown","source":"### <font color='darkred'>Random forest </font>"},{"metadata":{"id":"LzP9OFK1x3tx","trusted":true},"cell_type":"code","source":"# Applying countvectorizer on test_transform list that we created before \n\ntest_dataset = countvector.transform(test_transform)\npredictions = randomclassifier.predict(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"id":"_am2frIIx3t1","outputId":"1e79c5cc-3779-4ec5-ef85-e35438d43e4d","trusted":true},"cell_type":"code","source":"# confusion matrix for \n\nmatrix=confusion_matrix(test['Label'],predictions)\nprint(matrix)","execution_count":null,"outputs":[]},{"metadata":{"id":"PY7cJPtLx3t7","outputId":"090c4831-9ccd-4ef7-9a12-ed6ec81b1d1a","trusted":true},"cell_type":"code","source":"# accuracy score (compared test daset original output values with predictions)\n\nscore=accuracy_score(test['Label'],predictions)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Import library to check accuracy\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n\nmatrix=confusion_matrix(test['Label'],predictions)\nprint(matrix)\nscore=accuracy_score(test['Label'],predictions)\nprint(score)\nreport=classification_report(test['Label'],predictions)\nprint(report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='darkblue'>Lets apply XGBoost , and will also try different numbers of max features for countvectorizer and see which number gives us the maximum accuracy</font>\n\n\n\n\n### <font color='darkred'>XGBoost </font>"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"max_features_num = [500,600,700,800,900,1000]\nngram = [1,2,3,4,5]\nfor i in max_features_num:\n    for j in ngram:\n        countvector=CountVectorizer(ngram_range=(j,j),max_features=i)\n        traindataset=countvector.fit_transform(headlines)\n        test_dataset = countvector.transform(test_transform)\n\n        xgb = XGBClassifier(random_state =1)\n        xgb.fit(pd.DataFrame(traindataset.todense(), columns=countvector.get_feature_names()),train['Label'])\n        predictions = xgb.predict(pd.DataFrame(test_dataset.todense(), columns=countvector.get_feature_names()))\n        score=accuracy_score(test['Label'],predictions)\n        print('max number of features used : {}'.format(i))\n        print('ngram_range ({},{})'.format(j,j))\n        print(score)\n        matrix=confusion_matrix(test['Label'],predictions)\n        print('confusion matrix : {}'.format(matrix))\n        print('===============================')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='darkblue'>Maximum accuracy :</font>\n\nmax number of features used : 800\n\nngram_range (2,2)\n\n0.8650793650793651\n\nconfusion matrix : [[161  25]\n [ 26 166]]"},{"metadata":{"trusted":true},"cell_type":"code","source":"countvector=CountVectorizer(ngram_range=(1,1),max_features=800)\ntraindataset=countvector.fit_transform(headlines)\ntest_dataset = countvector.transform(test_transform)\n\n\nxgb = XGBClassifier(random_state =1)\nxgb.fit(pd.DataFrame(traindataset.todense(), columns=countvector.get_feature_names()),train['Label'])\npredictions = xgb.predict(pd.DataFrame(test_dataset.todense(), columns=countvector.get_feature_names()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix=confusion_matrix(test['Label'],predictions)\nprint(matrix)\nscore=accuracy_score(test['Label'],predictions)\nprint(score)\nreport=classification_report(test['Label'],predictions)\nprint(report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color='darkred'> Conclusion</font>"},{"metadata":{},"cell_type":"markdown","source":"\n\n<font color='darkblue'>After all this analysis we can conclude that the best algorithm which gave good accuracy and more true positive values and less on false negetive values then the best algorithm for you is Random Forest without hyperparameter tuning</font>"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}